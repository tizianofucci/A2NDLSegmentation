{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I5FnwOggnFIl"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tOBDSA9BnFIx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bns18Nm7oORk"
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z8bhsWQnnU-i"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Uw-loJ73n0eS"
   },
   "outputs": [],
   "source": [
    "#!unzip /content/drive/My\\ Drive/VOCDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z3gCzJ2NP3Rl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ASUS\\\\Documents\\\\UniversitÃ \\\\ANNDL\\\\A2NDLSegmentation\\\\Notebook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!ls /content/VOCDataset/Images\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH3lzlVqnFI2"
   },
   "source": [
    "# Example: Image Segmentation\n",
    "## Build segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4N88wG50nFI3"
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator\n",
    "# ------------------\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# Create training ImageDataGenerator object\n",
    "# We need two different generators for images and corresponding masks\n",
    "if apply_data_augmentation:\n",
    "    img_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                      width_shift_range=10,\n",
    "                                      height_shift_range=10,\n",
    "                                      zoom_range=0.3,\n",
    "                                      horizontal_flip=True,\n",
    "                                      vertical_flip=True,\n",
    "                                      fill_mode='reflect')\n",
    "    mask_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                       width_shift_range=10,\n",
    "                                       height_shift_range=10,\n",
    "                                       zoom_range=0.3,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True,\n",
    "                                       fill_mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hsieZk4aKhm6"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "  \"\"\"\n",
    "    CustomDataset inheriting from tf.keras.utils.Sequence.\n",
    "\n",
    "    3 main methods:\n",
    "      - __init__: save dataset params like directory, filenames..\n",
    "      - __len__: return the total number of samples in the dataset\n",
    "      - __getitem__: return a sample from the dataset\n",
    "\n",
    "    Note: \n",
    "      - the custom dataset return a single sample from the dataset. Then, we use \n",
    "        a tf.data.Dataset object to group samples into batches.\n",
    "      - in this case we have a different structure of the dataset in memory. \n",
    "        We have all the images in the same folder and the training and validation splits\n",
    "        are defined in text files.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
    "               preprocessing_function=None, out_shape=[256, 256]):\n",
    "    if which_subset == 'training':\n",
    "      subset_file = os.path.join(dataset_dir, 'Splits', 'train.txt')\n",
    "    elif which_subset == 'validation':\n",
    "      subset_file = os.path.join(dataset_dir, 'Splits', 'val.txt')\n",
    "    \n",
    "    with open(subset_file, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "    \n",
    "    subset_filenames = []\n",
    "    for line in lines:\n",
    "      subset_filenames.append(line.strip()) \n",
    "\n",
    "    self.which_subset = which_subset\n",
    "    self.dataset_dir = dataset_dir\n",
    "    self.subset_filenames = subset_filenames\n",
    "    self.img_generator = img_generator\n",
    "    self.mask_generator = mask_generator\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.out_shape = out_shape\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.subset_filenames)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Read Image\n",
    "    curr_filename = self.subset_filenames[index]\n",
    "    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\n",
    "    mask = Image.open(os.path.join(self.dataset_dir, 'Annotations', curr_filename + '.png'))\n",
    "\n",
    "    # Resize image and mask\n",
    "    img = img.resize(self.out_shape)\n",
    "    mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n",
    "    \n",
    "    img_arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    # in this dataset 255 mask label is assigned to an additional class, which corresponds \n",
    "    # to the contours of the objects. We remove it for simplicity.\n",
    "    mask_arr[mask_arr == 255] = 0  \n",
    "\n",
    "    mask_arr = np.expand_dims(mask_arr, -1)\n",
    "\n",
    "    if self.which_subset == 'training':\n",
    "      if self.img_generator is not None and self.mask_generator is not None:\n",
    "        # Perform data augmentation\n",
    "        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
    "        # and we can apply it to the image using apply_transform\n",
    "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
    "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
    "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
    "        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n",
    "        # Thus, when applied to the masks it will output 'interpolated classes', which\n",
    "        # is an unwanted behaviour. As a trick, we can transform each class mask \n",
    "        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n",
    "        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n",
    "        out_mask = np.zeros_like(mask_arr)\n",
    "        for c in np.unique(mask_arr):\n",
    "          if c > 0:\n",
    "            curr_class_arr = np.float32(mask_arr == c)\n",
    "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
    "            # from [0, 1] to {0, 1}\n",
    "            curr_class_arr = np.uint8(curr_class_arr)\n",
    "            # recover original class\n",
    "            curr_class_arr = curr_class_arr * c \n",
    "            out_mask += curr_class_arr\n",
    "    else:\n",
    "      out_mask = mask_arr\n",
    "    \n",
    "    if self.preprocessing_function is not None:\n",
    "        img_arr = self.preprocessing_function(img_arr)\n",
    "\n",
    "    return img_arr, np.float32(out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TyrdiIh_PWjB"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/VOCDataset\\\\Splits\\\\train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-af4891e5cc56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m dataset = CustomDataset('/content/VOCDataset', 'training', \n\u001b[0;32m      7\u001b[0m                         \u001b[0mimg_generator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_data_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_generator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_data_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                         preprocessing_function=preprocess_input)\n\u001b[0m\u001b[0;32m      9\u001b[0m dataset_valid = CustomDataset('/content/VOCDataset', 'validation', \n\u001b[0;32m     10\u001b[0m                               preprocessing_function=preprocess_input)\n",
      "\u001b[1;32m<ipython-input-9-3901c5a37da9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset_dir, which_subset, img_generator, mask_generator, preprocessing_function, out_shape)\u001b[0m\n\u001b[0;32m     27\u001b[0m       \u001b[0msubset_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Splits'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m       \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/VOCDataset\\\\Splits\\\\train.txt'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "dataset = CustomDataset('/content/VOCDataset', 'training', \n",
    "                        img_generator=img_data_gen, mask_generator=mask_data_gen,\n",
    "                        preprocessing_function=preprocess_input)\n",
    "dataset_valid = CustomDataset('/content/VOCDataset', 'validation', \n",
    "                              preprocessing_function=preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usz5SKPeQrOE"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n",
    "\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n",
    "valid_dataset = valid_dataset.batch(32)\n",
    "\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgpWhorBogdB"
   },
   "outputs": [],
   "source": [
    "!ls /content/VOCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOBabUbmnFJE"
   },
   "outputs": [],
   "source": [
    "# Let's test data generator\n",
    "# -------------------------\n",
    "import time\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Assign a color to each class\n",
    "evenly_spaced_interval = np.linspace(0, 1, 20)\n",
    "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
    "\n",
    "iterator = iter(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eunbPwWqPnB"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "augmented_img, target = next(iterator)\n",
    "augmented_img = augmented_img[0]   # First element\n",
    "augmented_img = augmented_img  # denormalize\n",
    "\n",
    "target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n",
    "\n",
    "print(np.unique(target))\n",
    "\n",
    "target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
    "\n",
    "target_img[np.where(target == 0)] = [0, 0, 0]\n",
    "for i in range(1, 21):\n",
    "  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\n",
    "\n",
    "ax[0].imshow(np.uint8(augmented_img))\n",
    "ax[1].imshow(np.uint8(target_img))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "553NrJXrFCZo"
   },
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "vgg.summary()\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA1TGlOMnFJM"
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvsdiF0TFTbt"
   },
   "outputs": [],
   "source": [
    "def create_model(depth, start_f, num_classes):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    # -------\n",
    "    model.add(vgg)\n",
    "    \n",
    "    start_f = 256\n",
    "        \n",
    "    # Decoder\n",
    "    # -------\n",
    "    for i in range(depth):\n",
    "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                         kernel_size=(3, 3),\n",
    "                                         strides=(1, 1),\n",
    "                                         padding='same'))\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "        start_f = start_f // 2\n",
    "\n",
    "    # Prediction Layer\n",
    "    # ----------------\n",
    "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n",
    "                                     kernel_size=(1, 1),\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',\n",
    "                                     activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k97OK6CRnFJS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(depth=5, \n",
    "                     start_f=8, \n",
    "                     num_classes=21)\n",
    "\n",
    "# Visualize created model as a table\n",
    "model.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "# model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzh16WTnFJW"
   },
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MlmYGVMnFJW"
   },
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Here we define the intersection over union for each class in the batch.\n",
    "# Then we compute the final iou as the mean over classes\n",
    "def meanIoU(y_true, y_pred):\n",
    "    # get predicted class from softmax\n",
    "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
    "\n",
    "    per_class_iou = []\n",
    "\n",
    "    for i in range(1,21): # exclude the background class 0\n",
    "      # Get prediction and target related to only a single class (i)\n",
    "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
    "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
    "      intersection = tf.reduce_sum(class_true * class_pred)\n",
    "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
    "    \n",
    "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "      per_class_iou.append(iou)\n",
    "\n",
    "    return tf.reduce_mean(per_class_iou)\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "metrics = ['accuracy', meanIoU]\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA6D_TBknFJZ"
   },
   "source": [
    "## Training with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XiwaKZhnFJa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'drive/My Drive/Keras4/', 'multiclass_segmentation_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=100,  #### set repeat in training dataset\n",
    "          steps_per_epoch=len(dataset),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(dataset_valid), \n",
    "          callbacks=callbacks)\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ1Yl-InnFJd"
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuVw1q_NnFJh"
   },
   "source": [
    "## Compute prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JUMMkS9eyQW"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/My Drive/Keras4/multiclass_segmentation_experiments/CNN_Nov27_08-41-36/ckpts/cp_02.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zo1JbCO5nFJh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "iterator = iter(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M75UjH7xxS7J"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
    "fig.show()\n",
    "image, target = next(iterator)\n",
    "\n",
    "image = image[0]\n",
    "target = target[0, ..., 0]\n",
    "\n",
    "out_sigmoid = model.predict(x=tf.expand_dims(image, 0))\n",
    "\n",
    "# Get predicted class as the index corresponding to the maximum value in the vector probability\n",
    "# predicted_class = tf.cast(out_sigmoid > score_th, tf.int32)\n",
    "# predicted_class = predicted_class[0, ..., 0]\n",
    "predicted_class = tf.argmax(out_sigmoid, -1)\n",
    "\n",
    "out_sigmoid.shape\n",
    "\n",
    "predicted_class = predicted_class[0, ...]\n",
    "\n",
    "# Assign colors (just for visualization)\n",
    "target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
    "prediction_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
    "\n",
    "target_img[np.where(target == 0)] = [0, 0, 0]\n",
    "for i in range(1, 21):\n",
    "  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\n",
    "\n",
    "prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\n",
    "for i in range(1, 21):\n",
    "  prediction_img[np.where(predicted_class == i)] = np.array(colors[i-1])[:3] * 255\n",
    "\n",
    "ax[0].imshow(np.uint8(image))\n",
    "ax[1].imshow(np.uint8(target_img))\n",
    "ax[2].imshow(np.uint8(prediction_img))\n",
    "\n",
    "fig.canvas.draw()\n",
    "time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Multiclass_Segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
